---
title: "A8a-ts_analysis_val"
author: "Jasper"
format: html
editor: visual
---

Script that performs the entire time series analysis (A2, A3, A4, A5, A6, A7) for the validation points. 

```{r}
library(tidyverse)
library(sf)
library(fable)
library(fabletools)
library(feasts)
library(data.table)
library(terra)
library(lubridate)
library(tsibble)
library(ggplot2)
library(scales)
library(zoo)
library(caret)
library(lme4)
library(ranger)
library(ROCit)
library(plotly)
library(viridis)
```

# Import and pre-process validation data

```{r}
# Add column validation data
val_pnts <- st_read('data/validation_points.GeoJSON')
val_pnts <- val_pnts %>% 
  arrange(country, regreened) %>% 
  mutate(plotID = seq(1,nrow(val)))

# Write validation point data as geojson and csv
st_write(obj=val_pnts, dsn='data/validation_points.GeoJSON', driver='GeoJSON')
st_write(obj=val_pnts, dsn='data/validation_points.csv', driver='CSV')
```

# Extract satellite information
Extract the landsat 8 and GPM satellite information using the GEE extract script (A2)


# Create reflectance dataframe for validation points
Function for cloud filtering

```{r}
cloud_landsat <- function(x){
  bits <- as.numeric(intToBits(x))[1:16]
  if(bits[7] == 1 && bits[5] == 0 && bits[3] == 0){
    return(0)
  }
  else(return(1))
}
```

Convert to reflectance and add dates

```{r}
refl_val <- fread('output/gee/Val_refl_l8.csv', select =c('date', 'SR_B4', 'SR_B5','QA_PIXEL','plotID'))  

refl_val$date <- as.Date(refl_val$date, tryFormats = "%d/%m/%Y")
  
# Scale DN values to reflectance values
scale_fac <- 0.0000275
offset <- -0.2
refl_val[,c('SR_B4', 'SR_B5')] <- (refl_val[,c('SR_B4', 'SR_B5')] * scale_fac) + offset

# Remove negative reflectance
refl_val <- refl_val[refl_val$SR_B4 >= 0,]
refl_val <- refl_val[refl_val$SR_B5 >= 0,]
  
# Remove clouds
refl_val$clouds <- 0
refl_val$clouds <- lapply(refl_val$QA_PIXEL, cloud_landsat) # Apply function to all pixels 
refl_val <- refl_val[refl_val$clouds==0,] # Filter out pixels with clouds

# Compute some VIs
refl_val$ndvi <- (refl_val$SR_B5 - refl_val$SR_B4) / (refl_val$SR_B5 + refl_val$SR_B4)
  
# Take average per validation point and date
#(This is to remove duplicates with the same date and plotID as a result of overlapping images and a single point falling in two images on the same date)
refl_aggr_val <- refl_val[,.(ndvi=mean(ndvi,na.rm=T)), by=list(plotID, date)]

# Add yearmon
refl_aggr_val$yearmon <- as.Date(ISOdate(year(refl_aggr_val$date), month(refl_aggr_val$date), 15))
```


# Extract the GPM at validation points
Import

```{r}
# gpm
gpm <- rast('output/gee/gpm/Val_GPM_stack.tif')

# convert validation points to terra:vector
val_pnts <- vect(val_pnts)
```

Set time series parameters

```{r}
# Set start/end
start_ts <- as.Date('2013-01-01')
end_ts <- as.Date('2023-01-15')

# Determine number of months in time series period
months_dif <- (year(end_ts) - year(start_ts)) * 12 + (month(end_ts) - month(start_ts))
months_ts <- seq(ymd(start_ts), by = "month", length.out=months_dif)
```

Extract GPM at validation points and assign date + plotID

```{r}
# Assign dates to gpm_stack layers
names(gpm) <- months_ts
  
#### Extract values for each layer
gpm_val <- data.table(extract(gpm, val_pnts, cells=F))
gpm_val <- gpm_val[,-c('ID')] # remove ID column
rownames(gpm_val) <- val_pnts$plotID

# Transpose df
gpm_ts <- cbind(row = as.integer(rownames(gpm_val)), stack(gpm_val))
  
# Clean df
names(gpm_ts) <- c('plotID', 'prcp_month', 'date')
gpm_ts$yearmon <- as.Date(ISOdate(year(gpm_ts$date), month(gpm_ts$date), 15))

# Concert val_pnts back to sf object
val_pnts <- st_as_sf(val_pnts)
```


# Add precipitation to reflectance ts
```{r}
# Convert val_pnts to datatable 
val_pnts_dt <- data.table(val_pnts)

# Merge reflectance ts with precipitation data
val_ts <- merge(refl_aggr_val, gpm_ts[,c('plotID', 'yearmon','prcp_month')],
               by=c('plotID', 'yearmon'), all.y=T)

# Write complete validation time series
fwrite(val_ts, 'output/time_series/Val_l8_ts.csv')
val_ts <- fread('output/time_series/Val_l8_ts.csv')
```


# Vegetation modeling
Cut off dates outside period of interest
```{r}
# Filter out time stamps before the observation
val_ts <- val_ts[val_ts$yearmon > as.Date('2013-01-01') & val_ts$yearmon < as.Date('2022-11-01')]
```


Create tsibble for analysis

```{r}
# Creates tsibble with lagged precipitation and interpolate the missing ndvi values
val_ts_pnt <- val_ts %>% 
  group_by(plotID, yearmon) %>% 
  summarize(ndvi=mean(ndvi, na.rm=T),
            prcp=mean(prcp_month)) %>% 
  # Add lagged precipitation variables
  mutate(prcp_lag1=data.table::shift(prcp, n=1, type='lag'),
         prcp_lag2=data.table::shift(prcp, n=2, type='lag')) %>% 
  # Check if plot ts is all NAs and exclude
  mutate(entire_na = length(which(!is.na(ndvi))) == 0) %>% 
  filter(!entire_na) %>% 
  # Exclude NAs when at beginning or end of time series
  slice(min(which(!is.na(ndvi))):max(which(!is.na(ndvi)))) %>% 
  # Remove the entire.na column
  select(!entire_na) %>% 
  # Interpolate NDVI values
  mutate(yearmonth=yearmonth(as.character(yearmon)),
         ndvi_int=na.approx(ndvi)) %>% 
  # Convert to tsibble
  as_tsibble(key=plotID, index=yearmonth)

fwrite(val_ts_pnt, 'output/time_series/Val_ts_pnt.csv')
val_ts_pnt <- read_csv('output/time_series/Val_ts_pnt.csv')

# Convert to tsibble after import
val_ts_pnt <- val_ts_pnt %>%  
  mutate(yearmonth=yearmonth(as.character(yearmon))) %>% 
  # Convert to tsibble
  as_tsibble(key=plotID, index=yearmonth)

# Create reference and validation sets
val_ref_pnt <- val_ts_pnt %>% filter(year(yearmonth) < 2017)
val_prd_pnt <- val_ts_pnt %>% filter(year(yearmonth) >= 2017)
```


Train ARIMA models and predict

```{r}
# Train ARIMA models
val_armax_pnt <- val_ref_pnt %>% 
  model(ARIMA(ndvi_int ~ prcp + prcp_lag1 + prcp_lag2 + pdq(d=0) + PDQ(D=0), stepwise = T, ic='aicc'))

# Forecast ndvi based on models and precipitation
val_fc_pnt <- fabletools::forecast(val_armax_pnt, new_data = val_prd_pnt[,c('yearmonth', 'prcp', 'prcp_lag1', 'prcp_lag2', 'plotID')])

# Extract 95% confidence levels
val_pnt_ci <- val_fc_pnt$ndvi_int %>% 
  hilo(80) 
val_fc_pnt <- val_fc_pnt %>% 
  mutate(upper = val_pnt_ci$upper,
         lower = val_pnt_ci$lower)

# Save the ARIMA models and forecast
save(val_fc_pnt, file = 'output/models/Val_l8_fc_pnt2.RDS')
save(val_armax_pnt, file = 'output/models/Val_l8_armax_pnt2.RDS')
load(file = 'output/models/Val_l8_fc_pnt2.RDS')
load(file = 'output/models/Val_l8_armax_pnt2.RDS')
```



# Interpret vegetation modeling
Prepare data

```{r}
# Select relevant rows and add yearmon column
val_fc_pnt_dt <- data.table(val_fc_pnt[,c('.mean', 'yearmonth', 'plotID', 'upper', 'lower')])
names(val_fc_pnt_dt)[names(val_fc_pnt_dt) == '.mean'] <- 'pred_test'
val_fc_pnt_dt$yearmon <- as.Date(ISOdate(year(val_fc_pnt_dt$yearmonth), month(val_fc_pnt_dt$yearmonth), 15))

# Merge actual ndvi with the predicted ndvi
val_fc_ts <- merge(val_ts_pnt, val_fc_pnt_dt[,-c('yearmonth')], by=c('yearmon', 'plotID'), all.x=T)
val_fc_ts$yearmonth <- yearmonth(val_fc_ts$yearmon)

# Merge actual ndvi with predicted reference ndvi
val_fc_ts <- merge(val_fc_ts, residuals(val_armax_pnt)[c('plotID', 'yearmonth', '.resid')], 
                   by=c('yearmonth', 'plotID'), all.x=T)

# Compute training prediction
val_fc_ts$pred_train <- val_fc_ts$ndvi_int + val_fc_ts$.resid

# Combine test and training prediction into one and convert to tsibble
val_fc_ts <- val_fc_ts %>% 
  tsibble(key=plotID, index=yearmonth) %>% 
  mutate(ndvi_pred = if_else(is.na(pred_test), pred_train, pred_test))

# Add validation data to time series
val_fc_ts <- val_fc_ts %>% 
  right_join(val_pnts[,c('plotID', 'regreened')], by='plotID') %>% 
  # For caret randomforest, regreened variable cannot start with a number
  mutate(regreened = if_else(regreened == 1, 'yes', 'no'))
```

Create dataset with MAE (and other stats) and plot

```{r}
val_mae <- val_fc_ts %>% 
  mutate(test_res = ndvi_int - upper,
         test_res_0 = if_else(ndvi_int > upper, ndvi_int - upper, 0),
         pred_resid = pred_test - ndvi_int,
         training = if_else(!is.na(pred_train), 1, 0)) %>% 
  tibble() %>% 
  group_by(plotID) %>% 
  summarise(regreened = first(regreened),
            plotID = first(plotID),
            ndvi=mean(ndvi_int),
            sd=sd(ndvi_int),
            n = n(),
            n_training = sum(training),
            mae = sum(test_res, na.rm=T) / (n - n_training),
            mae_0 = sum(test_res_0, na.rm=T) / (n - n_training) * 100,
            maep = sum((test_res/ndvi_int)*100, na.rm=T) / (n - n_training),
            rmse_test = sum(sqrt(pred_resid^2), na.rm=T) / (n - n_training),
            rmse_train = sum(sqrt(.resid^2), na.rm=T) / n_training) %>% 
  right_join(val_pnts[,c('plotID', 'country')], by='plotID')
```


Test accuracy of proposed method

```{r}
# Test accuracies of various MAE thresholds
interval <- seq(1, 1.5, by = 0.1)

for (i in interval){
  binary <- val_mae %>% 
    mutate(bin_class = if_else(mae_0 <= i, 'no', 'yes'))
  
  cm <- confusionMatrix(factor(binary$bin_class), 
                factor(binary$regreened), positive='yes', mode='everything')
  print(paste('The mean absolute error threshold is',i))
  print(cm)
}

# Best classification MAEp threshold = 1.4
binary <- val_mae %>% 
  #filter(country=='Rwanda') %>% 
  mutate(bin_class = if_else(mae_0 <= 1.4, 'no', 'yes'))

  
cm <- confusionMatrix(factor(binary$bin_class), 
                      factor(binary$regreened), 
                      positive='yes', mode='everything')

write.csv(as.matrix(cm,what='classes'), 'figures/acc_validation.csv')
write.csv(as.matrix(cm), 'figures/cm_validation.csv')
```